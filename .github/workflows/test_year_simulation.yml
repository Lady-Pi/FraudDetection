name: Test Full Year Simulation

on:
  workflow_dispatch:
    inputs:
      start_month:
        description: 'Starting month (01-12)'
        required: false
        default: '01'
        type: string
      end_month:
        description: 'Ending month (01-12)'
        required: false
        default: '12'
        type: string

env:
  PYTHON_VERSION: '3.11'
  MLFLOW_TRACKING_URI: file:./mlruns
  PERFORMANCE_THRESHOLD: 0.05

jobs:
  simulate-year:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Create directories
      run: |
        mkdir -p models/current models/archive metrics/artifacts logs mlruns

    - name: Verify monthly data
      run: |
        echo "Verifying existing monthly data structure:"
        for month in {01..12}; do
          if [ -d "data/monthly/$month" ]; then
            loans=$(wc -l < "data/monthly/$month/loan_applications.csv" 2>/dev/null || echo "0")
            trans=$(wc -l < "data/monthly/$month/transactions.csv" 2>/dev/null || echo "0")
            echo "Month $month: $loans loans, $trans transactions"
          else
            echo "Month $month: MISSING"
            exit 1
          fi
        done

    - name: Initialize baseline model
      run: |
        echo "Training initial baseline model (months 01-03)..."
        python -m src.models.train --through 03
        
        if [ -f "models/current/model.pkl" ]; then
          echo "Baseline model created successfully"
        else
          echo "Failed to create baseline model"
          exit 1
        fi

    - name: Simulate monthly evaluations
      run: |
        echo "Starting 12-month simulation..."
        echo "Month,Accuracy,AUROC,Performance_Drop,Needs_Retrain,Action_Taken" > simulation_results.csv
        
        START_MONTH=${{ github.event.inputs.start_month }}
        END_MONTH=${{ github.event.inputs.end_month }}
        
        for MONTH in $(seq -w ${START_MONTH#0} ${END_MONTH#0}); do
          echo ""
          echo "Evaluating Month $MONTH..."
          
          python -m src.models.evaluate --month $MONTH --output-json
          
          if [ -f "evaluation_results.json" ]; then
            ACCURACY=$(python -c "import json; print(json.load(open('evaluation_results.json'))['accuracy'])")
            AUROC=$(python -c "import json; print(json.load(open('evaluation_results.json'))['auroc'])")
            BASELINE_ACCURACY=$(python -c "import json; print(json.load(open('evaluation_results.json')).get('baseline_accuracy', 0.8))")
            
            ACCURACY_DROP=$(python -c "print($BASELINE_ACCURACY - $ACCURACY)")
            NEEDS_RETRAIN=$(python -c "print($ACCURACY_DROP >= ${{ env.PERFORMANCE_THRESHOLD }})")
            
            echo "  Accuracy: $ACCURACY"
            echo "  AUROC: $AUROC"
            echo "  Performance drop: $ACCURACY_DROP"
            
            if [ "$NEEDS_RETRAIN" == "True" ]; then
              echo "  RETRAINING TRIGGERED - Performance drop detected!"
              
              python -m src.models.train --through $MONTH
              
              if [ -f "models/current/model.pkl" ]; then
                echo "  Retraining completed for month $MONTH"
                ACTION_TAKEN="retrained"
                
                python -m src.models.evaluate --month $MONTH --output-json
                NEW_ACCURACY=$(python -c "import json; print(json.load(open('evaluation_results.json'))['accuracy'])")
                NEW_AUROC=$(python -c "import json; print(json.load(open('evaluation_results.json'))['auroc'])")
                echo "  New model accuracy: $NEW_ACCURACY"
                echo "  New model AUROC: $NEW_AUROC"
                
                ACCURACY=$NEW_ACCURACY
                AUROC=$NEW_AUROC
              else
                echo "  Retraining failed"
                ACTION_TAKEN="retrain_failed"
              fi
            else
              echo "  Model performance acceptable"
              ACTION_TAKEN="no_action"
            fi
            
            echo "$MONTH,$ACCURACY,$AUROC,$ACCURACY_DROP,$NEEDS_RETRAIN,$ACTION_TAKEN" >> simulation_results.csv
          else
            echo "  Evaluation failed for month $MONTH"
            echo "$MONTH,ERROR,ERROR,ERROR,ERROR,evaluation_failed" >> simulation_results.csv
          fi
          
          sleep 5
        done
        
        echo ""
        echo "Simulation Complete! Results:"
        echo "=================================="
        cat simulation_results.csv

    - name: Generate simulation report
      run: |
        python << 'EOF'
        import pandas as pd
        import json
        from datetime import datetime
        
        try:
            df = pd.read_csv('simulation_results.csv')
            
            total_months = len(df)
            retraining_months = len(df[df['Needs_Retrain'] == True])
            avg_accuracy = df[df['Accuracy'] != 'ERROR']['Accuracy'].astype(float).mean()
            
            retrained_months = df[df['Needs_Retrain'] == True]['Month'].astype(str).str.zfill(2).tolist()
            
            report = {
                "simulation_date": datetime.now().isoformat(),
                "total_months_evaluated": total_months,
                "retraining_triggered": retraining_months,
                "average_accuracy": round(avg_accuracy, 4),
                "retrained_months": retrained_months,
                "summary": f"Completed evaluation of {total_months} months with {retraining_months} retraining events"
            }
            
            with open('simulation_report.json', 'w') as f:
                json.dump(report, f, indent=2)
            
            print("SIMULATION REPORT GENERATED")
            print("=" * 50)
            print(f"Months evaluated: {total_months}")
            print(f"Retraining events: {retraining_months}")
            print(f"Average accuracy: {avg_accuracy:.4f}")
            print(f"Months that triggered retraining: {retrained_months}")
            
        except Exception as e:
            print(f"Error generating report: {e}")
        EOF

    - name: Upload simulation artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: year-simulation-results
        path: |
          simulation_results.csv
          simulation_report.json
          metrics/artifacts/
          metrics/performance_history.csv
        retention-days: 180

    - name: Summary Report
      if: always()
      run: |
        python << 'EOF'
        import pandas as pd
        
        try:
            df = pd.read_csv('simulation_results.csv')
            
            with open('summary_report.md', 'w') as f:
                f.write("## MLOps Pipeline - Full Year Simulation Results\n\n")
                
                # Basic overview
                total_months = len(df)
                retrained_months = df[df['Needs_Retrain'] == True]
                retrain_count = len(retrained_months)
                
                f.write("### Overview\n")
                f.write(f"- **Total months evaluated**: {total_months}\n")
                f.write(f"- **Retraining events**: {retrain_count}\n")
                if retrain_count > 0:
                    months = ', '.join([f"{int(m):02d}" for m in retrained_months['Month']])
                    f.write(f"- **Months retrained**: {months}\n")
                f.write("\n")
                
                # Detailed results table
                f.write("### Monthly Results\n")
                f.write("| Month | Pre-Retrain Accuracy | Final Accuracy | Performance Drop | Retrained |\n")
                f.write("|-------|---------------------|----------------|------------------|----------|\n")
                
                for _, row in df.iterrows():
                    month = f"{int(row['Month']):02d}"
                    
                    if row['Accuracy'] != 'ERROR':
                        final_accuracy = f"{float(row['Accuracy']):.3f}"
                        perf_drop = f"{float(row['Performance_Drop']):.4f}"
                        
                        # For retrained months, we need to show the original poor performance
                        if row['Needs_Retrain'] == True:
                            # Calculate pre-retrain accuracy from performance drop
                            baseline = 0.9207  # From baseline model training
                            pre_retrain_acc = baseline - float(row['Performance_Drop'])
                            pre_retrain = f"{pre_retrain_acc:.3f}"
                        else:
                            pre_retrain = "N/A"
                    else:
                        final_accuracy = "ERROR"
                        pre_retrain = "ERROR"
                        perf_drop = "ERROR"
                    
                    retrained = "Yes" if row['Needs_Retrain'] == True else "No"
                    f.write(f"| {month} | {pre_retrain} | {final_accuracy} | {perf_drop} | {retrained} |\n")
                
                print("Summary report generated successfully")
            
            # Copy to GitHub step summary
            with open('summary_report.md', 'r') as f:
                content = f.read()
            
            with open('${{ github.step_summary }}', 'a') as f:
                f.write(content)
                
        except Exception as e:
            print(f"Summary generation failed: {e}")
            with open('${{ github.step_summary }}', 'a') as f:
                f.write("## MLOps Pipeline Results\n\nSimulation completed - check artifacts for detailed results.\n")
        EOF